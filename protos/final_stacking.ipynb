{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# E-Style Real Estate Price Prediction\n",
    "Kaggle competition notebook featuring LightGBM with monotonic constraints, type-specific modeling, and RMSLE optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup Libraries & Configuration\n",
    "Import core libraries, fix random seeds, and define helpers for RMSLE tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If LightGBM is missing in your environment, uncomment the next line.\n",
    "# %pip install -q lightgbm xgboost catboost\n",
    "\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SEED = 2025\n",
    "N_SPLITS = 5\n",
    "TARGET_COL = \"TradePrice\"\n",
    "ID_COL = \"Id\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\")\n",
    "\n",
    "\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    \"\"\"Fix all relevant random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def rmsle(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Compute RMSLE while protecting against negative predictions.\"\"\"\n",
    "    y_true = np.clip(y_true, a_min=0, a_max=None)\n",
    "    y_pred = np.clip(y_pred, a_min=0, a_max=None)\n",
    "    return math.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def memory_info(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Return a human-readable memory usage string for quick diagnostics.\"\"\"\n",
    "    usage_mb = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    return f\"{usage_mb:,.2f} MB\"\n",
    "\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Datasets\n",
    "Read raw CSV files with consistent schema handling and sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().resolve()\n",
    "DATA_DIR = BASE_DIR.parent / \"input\" / \"estyle-community-competition-2025\"\n",
    "OUTPUT_DIR = BASE_DIR.parent / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_path = DATA_DIR / \"train.csv\"\n",
    "test_path = DATA_DIR / \"test.csv\"\n",
    "sample_submission_path = DATA_DIR / \"sample_submission.csv\"\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing train data at {train_path}\")\n",
    "\n",
    "train_df = pd.read_csv(train_path, low_memory=False)\n",
    "test_df = pd.read_csv(test_path, low_memory=False)\n",
    "sample_submission = pd.read_csv(sample_submission_path, low_memory=False)\n",
    "\n",
    "# ▼ ここから追加：指定カラムを削除\n",
    "cols_to_drop = [  \n",
    "    \"TimeToNearestStation\",\n",
    "]\n",
    "\n",
    "dropped_train = [c for c in cols_to_drop if c in train_df.columns]\n",
    "dropped_test  = [c for c in cols_to_drop if c in test_df.columns]\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "test_df  = test_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "print(f\"Dropped from train: {dropped_train}\")\n",
    "print(f\"Dropped from test:  {dropped_test}\")\n",
    "# ▲ 追加ここまで\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, memory: {memory_info(train_df)}\")\n",
    "print(f\"Test shape:  {test_df.shape}, memory: {memory_info(test_df)}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Basic Cleaning & Type Casting\n",
    "Align numerical dtypes and ensure train/test columns match before feature work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Strip whitespace from column names to avoid subtle mismatches.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_boolean_columns(df: pd.DataFrame, bool_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Ensure boolean indicator columns are stored as integers for LightGBM.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in bool_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"Int8\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def align_train_test(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Run basic normalization and confirm schema alignment.\"\"\"\n",
    "    bool_columns = [\n",
    "        \"AreaIsGreaterFlag\",\n",
    "        \"FrontageIsGreaterFlag\",\n",
    "        \"TotalFloorAreaIsGreaterFlag\",\n",
    "        \"PrewarBuilding\",\n",
    "    ]\n",
    "    train_clean = cast_boolean_columns(standardize_columns(train), bool_columns)\n",
    "    test_clean = cast_boolean_columns(standardize_columns(test), bool_columns)\n",
    "\n",
    "    missing_in_test = sorted(set(train_clean.columns) - set(test_clean.columns) - {TARGET_COL})\n",
    "    if missing_in_test:\n",
    "        print(\"Columns present in train but absent in test:\", missing_in_test)\n",
    "    return train_clean, test_clean\n",
    "\n",
    "\n",
    "train_df, test_df = align_train_test(train_df, test_df)\n",
    "print(\"Post-alignment train dtypes summary:\\n\", train_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Missing Value Handling & Flag Features\n",
    "Impute categorical gaps with `'unknown'` and add binary indicators for all missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_work = train_df.copy()\n",
    "test_work = test_df.copy()\n",
    "\n",
    "missing_summary = (\n",
    "    pd.DataFrame({\n",
    "        \"train_missing_ratio\": train_work.isna().mean(),\n",
    "        \"test_missing_ratio\": test_work.isna().mean(),\n",
    "    })\n",
    "    .sort_values(\"train_missing_ratio\", ascending=False)\n",
    ")\n",
    "\n",
    "missing_columns = [\n",
    "    col\n",
    "    for col in missing_summary.index\n",
    "    if (train_work[col].isna().any() if col in train_work.columns else False)\n",
    "    or (test_work[col].isna().any() if col in test_work.columns else False)\n",
    "]\n",
    "\n",
    "categorical_columns = sorted(\n",
    "    set(train_work.select_dtypes(include=[\"object\"]).columns)\n",
    "    | set(test_work.select_dtypes(include=[\"object\"]).columns)\n",
    ")\n",
    "\n",
    "def add_missing_indicators(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_missing_flag\"] = df[col].isna().astype(\"int8\")\n",
    "\n",
    "def fill_categorical_unknown(train_df: pd.DataFrame, test_df: pd.DataFrame, cat_cols: List[str]) -> None:\n",
    "    for col in cat_cols:\n",
    "        if col in train_df.columns:\n",
    "            train_df[col] = train_df[col].fillna(\"unknown\")\n",
    "        if col in test_df.columns:\n",
    "            test_df[col] = test_df[col].fillna(\"unknown\")\n",
    "\n",
    "def fill_numeric_with_median(train_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    numeric_cols = sorted(set(train_df.select_dtypes(include=[np.number]).columns))\n",
    "    for col in numeric_cols:\n",
    "        if col == TARGET_COL:\n",
    "            continue\n",
    "        median_value = train_df[col].median()\n",
    "        if np.isnan(median_value):\n",
    "            median_value = 0.0\n",
    "        train_df[col] = train_df[col].fillna(median_value)\n",
    "        if col in test_df.columns:\n",
    "            test_df[col] = test_df[col].fillna(median_value)\n",
    "\n",
    "add_missing_indicators(train_work, missing_columns)\n",
    "add_missing_indicators(test_work, missing_columns)\n",
    "fill_categorical_unknown(train_work, test_work, categorical_columns)\n",
    "# fill_numeric_with_median(train_work, test_work)\n",
    "\n",
    "print(\"Missing indicators added for\", len(missing_columns), \"columns.\")\n",
    "missing_summary.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_work.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 8. Categorical Encoding & Label Preparation\n",
    "Convert remaining object columns to categorical dtype and define target transformations for RMSLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_domain_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if {\"Year\", \"BuildingYear\"}.issubset(df.columns):\n",
    "        df[\"BuildingAge\"] = (df[\"Year\"] - df[\"BuildingYear\"]).clip(lower=0)\n",
    "        flag_col = \"BuildingYear_missing_flag\"\n",
    "        if flag_col in df.columns:\n",
    "            df.loc[df[flag_col] == 1, \"BuildingAge\"] = np.nan\n",
    "    if \"Area\" in df.columns:\n",
    "        df[\"Area_log\"] = np.log1p(df[\"Area\"])\n",
    "    if \"TotalFloorArea\" in df.columns:\n",
    "        df[\"TotalFloorArea_log\"] = np.log1p(df[\"TotalFloorArea\"])\n",
    "        df[\"FloorArea_to_Area\"] = df[\"TotalFloorArea\"] / (df[\"Area\"] + 1e-3)\n",
    "    if {\"Frontage\", \"Area\"}.issubset(df.columns):\n",
    "        df[\"Frontage_to_sqrtArea\"] = df[\"Frontage\"] / (np.sqrt(df[\"Area\"]) + 1e-3)\n",
    "    # if {\"MaxTimeToNearestStation\", \"MinTimeToNearestStation\"}.issubset(df.columns):\n",
    "    #     df[\"StationTimeRange\"] = df[\"MaxTimeToNearestStation\"] - df[\"MinTimeToNearestStation\"]\n",
    "    \n",
    "    # NOTE: district_buildyear aggregation features are now created inside CV loop\n",
    "    # to prevent data leakage\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# 地域性集約特徴量（CV内計算でリーク防止）\n",
    "# ============================================================\n",
    "\n",
    "def build_municipality_agg(\n",
    "    df_train: pd.DataFrame,\n",
    "    target_col: str = \"TradePrice\",\n",
    "    group_col: str = \"Municipality\",\n",
    "    smoothing: float = 50.0\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    市区町村単位で取引価格の平均・中央値を計算し、スムージングを適用。\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'smoothed_mean': {municipality: value, ...},\n",
    "            'median': {municipality: value, ...},\n",
    "            'count': {municipality: count, ...},\n",
    "            'global_mean': float,\n",
    "            'global_median': float\n",
    "        }\n",
    "    \"\"\"\n",
    "    global_mean = df_train[target_col].mean()\n",
    "    global_median = df_train[target_col].median()\n",
    "    \n",
    "    agg_df = df_train.groupby(group_col)[target_col].agg(\n",
    "        local_mean='mean',\n",
    "        local_median='median',\n",
    "        count='size'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # ベイジアンスムージング\n",
    "    agg_df['smoothed_mean'] = (\n",
    "        (agg_df['local_mean'] * agg_df['count'] + global_mean * smoothing)\n",
    "        / (agg_df['count'] + smoothing)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'smoothed_mean': agg_df.set_index(group_col)['smoothed_mean'].to_dict(),\n",
    "        'median': agg_df.set_index(group_col)['local_median'].to_dict(),\n",
    "        'count': agg_df.set_index(group_col)['count'].to_dict(),\n",
    "        'global_mean': global_mean,\n",
    "        'global_median': global_median\n",
    "    }\n",
    "\n",
    "\n",
    "def build_district_agg(\n",
    "    df_train: pd.DataFrame,\n",
    "    target_col: str = \"TradePrice\",\n",
    "    group_col: str = \"DistrictName\",\n",
    "    smoothing: float = 50.0\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    町丁目単位で取引価格の平均・中央値を計算し、スムージングを適用。\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'smoothed_mean': {district: value, ...},\n",
    "            'median': {district: value, ...},\n",
    "            'count': {district: count, ...},\n",
    "            'global_mean': float,\n",
    "            'global_median': float\n",
    "        }\n",
    "    \"\"\"\n",
    "    global_mean = df_train[target_col].mean()\n",
    "    global_median = df_train[target_col].median()\n",
    "    \n",
    "    agg_df = df_train.groupby(group_col)[target_col].agg(\n",
    "        local_mean='mean',\n",
    "        local_median='median',\n",
    "        count='size'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # ベイジアンスムージング\n",
    "    agg_df['smoothed_mean'] = (\n",
    "        (agg_df['local_mean'] * agg_df['count'] + global_mean * smoothing)\n",
    "        / (agg_df['count'] + smoothing)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'smoothed_mean': agg_df.set_index(group_col)['smoothed_mean'].to_dict(),\n",
    "        'median': agg_df.set_index(group_col)['local_median'].to_dict(),\n",
    "        'count': agg_df.set_index(group_col)['count'].to_dict(),\n",
    "        'global_mean': global_mean,\n",
    "        'global_median': global_median\n",
    "    }\n",
    "\n",
    "\n",
    "def apply_location_aggregations(\n",
    "    df: pd.DataFrame,\n",
    "    municipality_agg_dict: dict,\n",
    "    district_agg_dict: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    事前計算した地域集約特徴量をデータフレームに適用。\n",
    "    \n",
    "    Args:\n",
    "        df: 特徴量を追加するDataFrame\n",
    "        municipality_agg_dict: 市区町村集約辞書\n",
    "        district_agg_dict: 町丁目集約辞書\n",
    "    \n",
    "    Returns:\n",
    "        特徴量が追加されたDataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 市区町村レベルの特徴量（カテゴリ型の制約を回避するため.astype(str)で文字列化）\n",
    "    if 'Municipality' in df.columns:\n",
    "        # カテゴリ型を一時的に文字列に変換してマッピング\n",
    "        muni_str = df['Municipality'].astype(str)\n",
    "        \n",
    "        # df['Municipality_price_mean'] = muni_str.map(\n",
    "        #     municipality_agg_dict['smoothed_mean']\n",
    "        # ).fillna(municipality_agg_dict['global_mean']).astype(float)\n",
    "        \n",
    "        df['Municipality_price_median'] = muni_str.map(\n",
    "            municipality_agg_dict['median']\n",
    "        ).fillna(municipality_agg_dict['global_median']).astype(float)\n",
    "        \n",
    "        # df['Municipality_count'] = muni_str.map(\n",
    "        #     municipality_agg_dict['count']\n",
    "        # ).fillna(0).astype(float)\n",
    "    \n",
    "    # 町丁目レベルの特徴量\n",
    "    if 'DistrictName' in df.columns:\n",
    "        # カテゴリ型を一時的に文字列に変換してマッピング\n",
    "        dist_str = df['DistrictName'].astype(str)\n",
    "        \n",
    "        # df['DistrictName_price_mean'] = dist_str.map(\n",
    "        #     district_agg_dict['smoothed_mean']\n",
    "        # ).fillna(district_agg_dict['global_mean']).astype(float)\n",
    "        \n",
    "        # df['DistrictName_price_median'] = dist_str.map(\n",
    "        #     district_agg_dict['median']\n",
    "        # ).fillna(district_agg_dict['global_median']).astype(float)\n",
    "        \n",
    "    #     df['DistrictName_count'] = dist_str.map(\n",
    "    #         district_agg_dict['count']\n",
    "    #     ).fillna(0).astype(float)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_filtered = add_domain_features(train_work)\n",
    "test_work = add_domain_features(test_work)\n",
    "\n",
    "fill_numeric_with_median(train_filtered, test_work)\n",
    "\n",
    "categorical_cols_final = sorted(\n",
    "    set(train_filtered.select_dtypes(include=[\"object\"]).columns)\n",
    "    | set(test_work.select_dtypes(include=[\"object\"]).columns)\n",
    ")\n",
    "for col in categorical_cols_final:\n",
    "    if col in train_filtered.columns:\n",
    "        train_filtered[col] = train_filtered[col].astype(\"category\")\n",
    "    if col in test_work.columns:\n",
    "        test_work[col] = test_work[col].astype(\"category\")\n",
    "\n",
    "train_target = train_filtered[TARGET_COL].copy()\n",
    "print(\"Categorical columns prepared:\", len(categorical_cols_final))\n",
    "print(\"Note: Aggregation features will be created inside CV loop (no leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 9. Segment Datasets by Property Type\n",
    "Split samples into `land only` and `with building` segments to train specialized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAND_KEYWORDS = [\"land\", \"土地\", \"宅地\", \"lot\", \"residential land\", \"commercial land\"]\n",
    "\n",
    "\n",
    "def detect_land_only(type_series: pd.Series) -> pd.Series:\n",
    "    type_str = type_series.astype(str).str.lower()\n",
    "    pattern = \"|\".join(LAND_KEYWORDS)\n",
    "    land_mask = type_str.str.contains(pattern, case=False, na=False)\n",
    "    return land_mask\n",
    "\n",
    "\n",
    "train_filtered[\"is_land_only\"] = detect_land_only(train_filtered[\"Type\"]).astype(\"int8\")\n",
    "test_work[\"is_land_only\"] = detect_land_only(test_work[\"Type\"]).astype(\"int8\")\n",
    "\n",
    "train_filtered[\"Type\"] = train_filtered[\"Type\"].cat.remove_unused_categories()\n",
    "\n",
    "train_filtered[\"is_land_only\"].value_counts(normalize=True).rename(\"share\").to_frame(\"share\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 10. LightGBM Monotonic Constraint Definitions\n",
    "Enforce domain knowledge (e.g., larger area ⇒ higher price) via monotone constraints per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONOTONIC_FEATURE_MAP = {\n",
    "    \"Area\": 1,\n",
    "    \"Area_log\": 1,\n",
    "    \"TotalFloorArea\": 1,\n",
    "    \"TotalFloorArea_log\": 1,\n",
    "    \"FloorArea_to_Area\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "def build_monotonic_constraints(feature_names: List[str]) -> str:\n",
    "    \"\"\"Return LightGBM-compatible monotone constraint string.\"\"\"\n",
    "    constraints = [MONOTONIC_FEATURE_MAP.get(name, 0) for name in feature_names]\n",
    "    return \"(\" + \",\".join(str(int(val)) for val in constraints) + \")\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 10.5. Stacking Strategy Implementation\n",
    "Implement a two-level stacking ensemble with diverse base models (LightGBM, XGBoost, CatBoost, ElasticNet, RandomForest) and an ElasticNet meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stacking_features(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Prepare features for stacking models (handle categoricals for ElasticNet/RF).\"\"\"\n",
    "    train_stack = train.copy()\n",
    "    test_stack = test.copy()\n",
    "    \n",
    "    # For ElasticNet and RandomForest, we need to encode categorical variables\n",
    "    categorical_cols = [col for col in train_stack.columns if str(train_stack[col].dtype) == \"category\"]\n",
    "    \n",
    "    # Convert categories to codes (simple encoding)\n",
    "    for col in categorical_cols:\n",
    "        if col in train_stack.columns:\n",
    "            train_stack[col] = train_stack[col].cat.codes\n",
    "        if col in test_stack.columns:\n",
    "            test_stack[col] = test_stack[col].cat.codes\n",
    "    \n",
    "    return train_stack, test_stack\n",
    "\n",
    "\n",
    "def train_level1_model(\n",
    "    model_name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    segment_name: str,\n",
    "    seed: int = SEED,\n",
    "    n_splits: int = N_SPLITS,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Train a single Level-1 model and return OOF and test predictions.\"\"\"\n",
    "    \n",
    "    oof_pred = np.zeros(len(X_train))\n",
    "    test_pred = np.zeros(len(X_test))\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    print(f\"  Training {model_name}...\")\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train), start=1):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n",
    "        \n",
    "        if model_name == \"LightGBM\":\n",
    "            cat_features = [col for col in X_train.columns if str(X_train[col].dtype) == \"category\"]\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                \"learning_rate\": 0.03,\n",
    "                \"n_estimators\": 3000,\n",
    "                \"num_leaves\": 128,\n",
    "                \"max_depth\": -1,\n",
    "                \"subsample\": 0.8,\n",
    "                \"subsample_freq\": 1,\n",
    "                \"colsample_bytree\": 0.8,\n",
    "                \"reg_alpha\": 0.1,\n",
    "                \"reg_lambda\": 0.1,\n",
    "                \"min_child_samples\": 50,\n",
    "                \"random_state\": seed + fold,\n",
    "                \"n_jobs\": -1,\n",
    "                \"verbose\": -1,\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                categorical_feature=cat_features,\n",
    "                callbacks=[lgb.early_stopping(150, verbose=False)],\n",
    "            )\n",
    "            oof_pred[valid_idx] = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "            test_pred += model.predict(X_test, num_iteration=model.best_iteration_) / n_splits\n",
    "            \n",
    "        elif model_name == \"XGBoost\":\n",
    "            # Prepare data for XGBoost (encode categories)\n",
    "            X_tr_xgb, X_val_xgb = X_tr.copy(), X_val.copy()\n",
    "            X_test_xgb = X_test.copy()\n",
    "            for col in X_tr_xgb.columns:\n",
    "                if str(X_tr_xgb[col].dtype) == \"category\":\n",
    "                    X_tr_xgb[col] = X_tr_xgb[col].cat.codes\n",
    "                    X_val_xgb[col] = X_val_xgb[col].cat.codes\n",
    "                    X_test_xgb[col] = X_test_xgb[col].cat.codes\n",
    "            \n",
    "            # XGBoost with early stopping using callbacks (compatible with latest versions)\n",
    "            params = {\n",
    "                \"objective\": \"reg:squarederror\",\n",
    "                \"learning_rate\": 0.03,\n",
    "                \"max_depth\": 7,\n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8,\n",
    "                \"reg_alpha\": 0.1,\n",
    "                \"reg_lambda\": 0.1,\n",
    "                \"random_state\": seed + fold,\n",
    "                \"n_jobs\": -1,\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params, n_estimators=3000)\n",
    "            model.fit(\n",
    "                X_tr_xgb, y_tr,\n",
    "                eval_set=[(X_val_xgb, y_val)],\n",
    "                verbose=False,\n",
    "            )\n",
    "            # Get best iteration if available, otherwise use all estimators\n",
    "            best_iter = getattr(model, 'best_iteration', None)\n",
    "            if best_iter is not None:\n",
    "                oof_pred[valid_idx] = model.predict(X_val_xgb, iteration_range=(0, best_iter + 1))\n",
    "                test_pred += model.predict(X_test_xgb, iteration_range=(0, best_iter + 1)) / n_splits\n",
    "            else:\n",
    "                oof_pred[valid_idx] = model.predict(X_val_xgb)\n",
    "                test_pred += model.predict(X_test_xgb) / n_splits\n",
    "            \n",
    "        elif model_name == \"CatBoost\":\n",
    "            cat_features = [i for i, col in enumerate(X_train.columns) if str(X_train[col].dtype) == \"category\"]\n",
    "            params = {\n",
    "                \"iterations\": 3000,\n",
    "                \"learning_rate\": 0.03,\n",
    "                \"depth\": 7,\n",
    "                \"l2_leaf_reg\": 3,\n",
    "                \"random_seed\": seed + fold,\n",
    "                \"verbose\": False,\n",
    "                \"early_stopping_rounds\": 150,\n",
    "            }\n",
    "            model = cb.CatBoostRegressor(**params)\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=(X_val, y_val),\n",
    "                cat_features=cat_features,\n",
    "                verbose=False,\n",
    "            )\n",
    "            oof_pred[valid_idx] = model.predict(X_val)\n",
    "            test_pred += model.predict(X_test) / n_splits\n",
    "            \n",
    "        elif model_name == \"ElasticNet\":\n",
    "            # Encode categories for ElasticNet\n",
    "            X_tr_en, X_val_en = X_tr.copy(), X_val.copy()\n",
    "            X_test_en = X_test.copy()\n",
    "            for col in X_tr_en.columns:\n",
    "                if str(X_tr_en[col].dtype) == \"category\":\n",
    "                    X_tr_en[col] = X_tr_en[col].cat.codes\n",
    "                    X_val_en[col] = X_val_en[col].cat.codes\n",
    "                    X_test_en[col] = X_test_en[col].cat.codes\n",
    "            \n",
    "            # Standardize features\n",
    "            scaler = StandardScaler()\n",
    "            X_tr_scaled = scaler.fit_transform(X_tr_en)\n",
    "            X_val_scaled = scaler.transform(X_val_en)\n",
    "            X_test_scaled = scaler.transform(X_test_en)\n",
    "            \n",
    "            # ElasticNet with balanced L1 and L2 regularization\n",
    "            model = ElasticNet(alpha=0.5, l1_ratio=0.5, max_iter=5000, random_state=seed + fold)\n",
    "            model.fit(X_tr_scaled, y_tr)\n",
    "            oof_pred[valid_idx] = model.predict(X_val_scaled)\n",
    "            test_pred += model.predict(X_test_scaled) / n_splits\n",
    "            \n",
    "        elif model_name == \"RandomForest\":\n",
    "            # Encode categories for RandomForest\n",
    "            X_tr_rf, X_val_rf = X_tr.copy(), X_val.copy()\n",
    "            X_test_rf = X_test.copy()\n",
    "            for col in X_tr_rf.columns:\n",
    "                if str(X_tr_rf[col].dtype) == \"category\":\n",
    "                    X_tr_rf[col] = X_tr_rf[col].cat.codes\n",
    "                    X_val_rf[col] = X_val_rf[col].cat.codes\n",
    "                    X_test_rf[col] = X_test_rf[col].cat.codes\n",
    "            \n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=15,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                max_features=\"sqrt\",\n",
    "                random_state=seed + fold,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            model.fit(X_tr_rf, y_tr)\n",
    "            oof_pred[valid_idx] = model.predict(X_val_rf)\n",
    "            test_pred += model.predict(X_test_rf) / n_splits\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate OOF score\n",
    "    oof_score = rmsle(np.expm1(y_train), np.maximum(np.expm1(oof_pred), 0))\n",
    "    print(f\"    {model_name} OOF RMSLE: {oof_score:.5f}\")\n",
    "    \n",
    "    return {\n",
    "        \"oof\": oof_pred,\n",
    "        \"test_pred\": test_pred,\n",
    "        \"score\": oof_score,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_stacking_cv(\n",
    "    train_segment: pd.DataFrame,\n",
    "    test_segment: pd.DataFrame,\n",
    "    segment_name: str,\n",
    "    seed: int = SEED,\n",
    "    n_splits: int = N_SPLITS,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"Run full stacking pipeline for a segment.\"\"\"\n",
    "    \n",
    "    features = get_feature_columns(train_segment)\n",
    "    X = train_segment[features]\n",
    "    y = np.log1p(train_segment[TARGET_COL].values)\n",
    "    X_test = test_segment[features]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Level-1 models for {segment_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Define Level-1 models\n",
    "    level1_models = [\"LightGBM\", \"XGBoost\", \"CatBoost\", \"ElasticNet\", \"RandomForest\"]\n",
    "    \n",
    "    # Train Level-1 models and collect OOF predictions\n",
    "    level1_results = {}\n",
    "    oof_predictions = np.zeros((len(X), len(level1_models)))\n",
    "    test_predictions = np.zeros((len(X_test), len(level1_models)))\n",
    "    \n",
    "    for i, model_name in enumerate(level1_models):\n",
    "        result = train_level1_model(model_name, X, y, X_test, segment_name, seed, n_splits)\n",
    "        level1_results[model_name] = result\n",
    "        oof_predictions[:, i] = result[\"oof\"]\n",
    "        test_predictions[:, i] = result[\"test_pred\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Level-2 Meta-Model (ElasticNet) for {segment_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train Level-2 meta-model (ElasticNet regression)\n",
    "    meta_oof = np.zeros(len(X))\n",
    "    meta_test = np.zeros(len(X_test))\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(oof_predictions), start=1):\n",
    "        X_meta_train = oof_predictions[train_idx]\n",
    "        y_meta_train = y[train_idx]\n",
    "        X_meta_valid = oof_predictions[valid_idx]\n",
    "        \n",
    "        # Standardize meta-features\n",
    "        scaler = StandardScaler()\n",
    "        X_meta_train_scaled = scaler.fit_transform(X_meta_train)\n",
    "        X_meta_valid_scaled = scaler.transform(X_meta_valid)\n",
    "        test_pred_scaled = scaler.transform(test_predictions)\n",
    "        \n",
    "        # Train meta-model with ElasticNet\n",
    "        meta_model = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000, random_state=seed + fold)\n",
    "        meta_model.fit(X_meta_train_scaled, y_meta_train)\n",
    "        \n",
    "        meta_oof[valid_idx] = meta_model.predict(X_meta_valid_scaled)\n",
    "        meta_test += meta_model.predict(test_pred_scaled) / n_splits\n",
    "    \n",
    "    # Calculate final scores\n",
    "    meta_oof_score = rmsle(train_segment[TARGET_COL].values, np.maximum(np.expm1(meta_oof), 0))\n",
    "    \n",
    "    # Compare with simple average\n",
    "    simple_avg_oof = oof_predictions.mean(axis=1)\n",
    "    simple_avg_score = rmsle(train_segment[TARGET_COL].values, np.maximum(np.expm1(simple_avg_oof), 0))\n",
    "    \n",
    "    print(f\"\\n  Meta-Model (ElasticNet) RMSLE: {meta_oof_score:.5f}\")\n",
    "    print(f\"  Simple Average RMSLE: {simple_avg_score:.5f}\")\n",
    "    print(f\"  Improvement: {simple_avg_score - meta_oof_score:.5f}\")\n",
    "    \n",
    "    return {\n",
    "        \"level1_results\": level1_results,\n",
    "        \"oof\": pd.Series(np.maximum(np.expm1(meta_oof), 0), index=train_segment.index, name=f\"oof_{segment_name}\"),\n",
    "        \"test_pred\": pd.Series(np.maximum(np.expm1(meta_test), 0), index=test_segment.index, name=f\"pred_{segment_name}\"),\n",
    "        \"meta_score\": meta_oof_score,\n",
    "        \"simple_avg_score\": simple_avg_score,\n",
    "        \"level1_model_names\": level1_models,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 11. K-Fold Cross-Validation Workflow\n",
    "Train LightGBM models per segment with RMSLE-focused validation and constraint-aware parameters.\n",
    "**NEW:** Option to use stacking ensemble with multiple diverse models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_FEATURES = {TARGET_COL, ID_COL, \"district_buildyear_price_count\", \"is_land_only\"}\n",
    "\n",
    "LIGHTGBM_PARAMS_BASE = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"n_estimators\": 5000,\n",
    "    \"num_leaves\": 128,\n",
    "    \"max_depth\": -1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"min_child_samples\": 50,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "\n",
    "def get_feature_columns(df: pd.DataFrame) -> List[str]:\n",
    "    return [col for col in df.columns if col not in EXCLUDE_FEATURES]\n",
    "\n",
    "\n",
    "def run_segment_cv(\n",
    "    train_segment: pd.DataFrame,\n",
    "    test_segment: pd.DataFrame,\n",
    "    segment_name: str,\n",
    "    seed: int = SEED,\n",
    "    n_splits: int = N_SPLITS,\n",
    "    retrain_on_full: bool = True,\n",
    ") -> Dict[str, object]:\n",
    "    features = get_feature_columns(train_segment)\n",
    "    cat_features = [col for col in features if str(train_segment[col].dtype) == \"category\"]\n",
    "    monotone_constraints = build_monotonic_constraints(features)\n",
    "\n",
    "    X = train_segment[features]\n",
    "    y = np.log1p(train_segment[TARGET_COL].values)\n",
    "    X_test = test_segment[features]\n",
    "\n",
    "    oof_pred = np.zeros(len(train_segment))\n",
    "    test_pred = np.zeros(len(test_segment))\n",
    "    fold_scores = []\n",
    "    feature_importances = []\n",
    "    best_iterations = []\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), start=1):\n",
    "        # CV内で地域性集約特徴量を計算（データリーク防止）\n",
    "        print(f\"    Fold {fold}: Computing location aggregation features...\")\n",
    "        train_fold_df = train_segment.iloc[train_idx].copy()\n",
    "        valid_fold_df = train_segment.iloc[valid_idx].copy()\n",
    "        \n",
    "        # 訓練フォールドから集約統計を計算\n",
    "        municipality_agg = build_municipality_agg(train_fold_df, target_col=TARGET_COL)\n",
    "        district_agg = build_district_agg(train_fold_df, target_col=TARGET_COL)\n",
    "        \n",
    "        # 訓練・検証データに集約特徴量を適用\n",
    "        train_fold_df = apply_location_aggregations(train_fold_df, municipality_agg, district_agg)\n",
    "        valid_fold_df = apply_location_aggregations(valid_fold_df, municipality_agg, district_agg)\n",
    "        \n",
    "        # 特徴量列を再取得（新しい集約特徴量を含む）\n",
    "        features_with_agg = get_feature_columns(train_fold_df)\n",
    "        cat_features_with_agg = [col for col in features_with_agg if str(train_fold_df[col].dtype) == \"category\"]\n",
    "        \n",
    "        X_train = train_fold_df[features_with_agg]\n",
    "        X_valid = valid_fold_df[features_with_agg]\n",
    "        y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "\n",
    "        params = LIGHTGBM_PARAMS_BASE.copy()\n",
    "        params.update({\"monotone_constraints\": monotone_constraints, \"random_state\": seed + fold})\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\",\n",
    "            categorical_feature=cat_features,\n",
    "            callbacks=[lgb.early_stopping(200), lgb.log_evaluation(200)],\n",
    "        )\n",
    "\n",
    "        best_iterations.append(model.best_iteration_)\n",
    "        \n",
    "        val_pred = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "        oof_pred[valid_idx] = np.maximum(np.expm1(val_pred), 0)\n",
    "\n",
    "        if not retrain_on_full:\n",
    "            # テストデータにも同じ集約特徴量を適用\n",
    "            test_fold = apply_location_aggregations(test_segment.copy(), municipality_agg, district_agg)\n",
    "            X_test_fold = test_fold[features_with_agg]\n",
    "            test_pred += np.maximum(\n",
    "                np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)),\n",
    "                0,\n",
    "            ) / n_splits\n",
    "\n",
    "        fold_score = rmsle(train_segment.iloc[valid_idx][TARGET_COL].values, oof_pred[valid_idx])\n",
    "        fold_scores.append(fold_score)\n",
    "\n",
    "        fold_importance = pd.DataFrame({\n",
    "            \"feature\": features,\n",
    "            \"importance\": model.booster_.feature_importance(importance_type=\"gain\"),\n",
    "            \"fold\": fold,\n",
    "            \"segment\": segment_name,\n",
    "        })\n",
    "        feature_importances.append(fold_importance)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # Retrain on full data if requested\n",
    "    if retrain_on_full:\n",
    "        print(f\"  Retraining {segment_name} on full training data...\")\n",
    "        avg_best_iteration = int(np.mean(best_iterations))\n",
    "        \n",
    "        # 全訓練データから集約特徴量を計算\n",
    "        print(f\"    Computing location aggregation features on full training data...\")\n",
    "        full_municipality_agg = build_municipality_agg(train_segment, target_col=TARGET_COL)\n",
    "        full_district_agg = build_district_agg(train_segment, target_col=TARGET_COL)\n",
    "        \n",
    "        # 訓練データに適用\n",
    "        train_full_with_agg = apply_location_aggregations(train_segment.copy(), full_municipality_agg, full_district_agg)\n",
    "        features_full = get_feature_columns(train_full_with_agg)\n",
    "        cat_features_full = [col for col in features_full if str(train_full_with_agg[col].dtype) == \"category\"]\n",
    "        X_full = train_full_with_agg[features_full]\n",
    "\n",
    "        params_full = LIGHTGBM_PARAMS_BASE.copy()\n",
    "        params_full.update({\n",
    "            \"monotone_constraints\": monotone_constraints,\n",
    "            \"random_state\": seed,\n",
    "            \"n_estimators\": avg_best_iteration + 50,\n",
    "        })\n",
    "        \n",
    "        final_model = lgb.LGBMRegressor(**params_full)\n",
    "        final_model.fit(\n",
    "            X, y,\n",
    "            categorical_feature=cat_features,\n",
    "            callbacks=[lgb.log_evaluation(200)],\n",
    "        )\n",
    "        \n",
    "        # テストデータに適用\n",
    "        test_with_agg = apply_location_aggregations(test_segment.copy(), full_municipality_agg, full_district_agg)\n",
    "        X_test_full = test_with_agg[features_full]\n",
    "        test_pred = np.maximum(\n",
    "            np.expm1(final_model.predict(X_test_full)),\n",
    "            0,\n",
    "        )\n",
    "\n",
    "    result = {\n",
    "        \"oof\": pd.Series(oof_pred, index=train_segment.index, name=f\"oof_{segment_name}\"),\n",
    "        \"test_pred\": pd.Series(test_pred, index=test_segment.index, name=f\"pred_{segment_name}\"),\n",
    "        \"score_mean\": np.mean(fold_scores),\n",
    "        \"score_std\": np.std(fold_scores),\n",
    "        \"feature_importances\": pd.concat(feature_importances, ignore_index=True),\n",
    "        \"avg_best_iteration\": int(np.mean(best_iterations)),\n",
    "    }\n",
    "    print(f\"Segment {segment_name}: RMSLE {result['score_mean']:.5f} ± {result['score_std']:.5f}\")\n",
    "    if retrain_on_full:\n",
    "        print(f\"  Avg best iteration: {result['avg_best_iteration']}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 12. Train Segment Models with Optional Stacking\n",
    "Execute segmented cross-validation with either single LightGBM or full stacking ensemble.\n",
    "Set `USE_STACKING=True` to enable multi-model stacking with ElasticNet meta-learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose training strategy:\n",
    "# - USE_STACKING=True: Use 5-model stacking ensemble (LightGBM, XGBoost, CatBoost, ElasticNet, RandomForest)\n",
    "# - USE_STACKING=False: Use single LightGBM model with optional full retrain\n",
    "USE_STACKING = True\n",
    "\n",
    "# For single LightGBM only (ignored if USE_STACKING=True):\n",
    "# - retrain_on_full=True: Retrain on all data after CV (recommended, uses more data)\n",
    "# - retrain_on_full=False: Average predictions from CV folds (more robust to overfitting)\n",
    "USE_FULL_RETRAIN = True\n",
    "\n",
    "segment_mapping = {1: \"land_only\", 0: \"with_building\"}\n",
    "segment_results = {}\n",
    "all_feature_importances = []\n",
    "\n",
    "oof_series = pd.Series(index=train_filtered.index, dtype=float)\n",
    "test_predictions_series = pd.Series(index=test_work.index, dtype=float)\n",
    "\n",
    "if USE_STACKING:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STACKING ENSEMBLE MODE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Level-1 Models: LightGBM, XGBoost, CatBoost, ElasticNet, RandomForest\")\n",
    "    print(\"Level-2 Meta-Model: ElasticNet Regression\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"SINGLE LIGHTGBM MODE\")\n",
    "    print(f\"Full Retrain: {USE_FULL_RETRAIN}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "for segment_value, segment_name in segment_mapping.items():\n",
    "    train_segment = train_filtered[train_filtered[\"is_land_only\"] == segment_value].copy()\n",
    "    test_segment = test_work[test_work[\"is_land_only\"] == segment_value].copy()\n",
    "\n",
    "    if train_segment.empty:\n",
    "        print(f\"Segment {segment_name} has no training records; skipping.\")\n",
    "        continue\n",
    "\n",
    "    if test_segment.empty:\n",
    "        print(f\"Segment {segment_name} has no test records; predictions will remain NaN.\")\n",
    "\n",
    "    if USE_STACKING:\n",
    "        # Use stacking ensemble\n",
    "        result = run_stacking_cv(train_segment, test_segment, segment_name)\n",
    "        segment_results[segment_name] = result\n",
    "    else:\n",
    "        # Use single LightGBM\n",
    "        result = run_segment_cv(train_segment, test_segment, segment_name, retrain_on_full=USE_FULL_RETRAIN)\n",
    "        segment_results[segment_name] = result\n",
    "        all_feature_importances.append(result[\"feature_importances\"])\n",
    "\n",
    "    oof_series.loc[train_segment.index] = result[\"oof\"]\n",
    "    if not test_segment.empty:\n",
    "        test_predictions_series.loc[test_segment.index] = result[\"test_pred\"]\n",
    "\n",
    "valid_oof = oof_series.dropna()\n",
    "overall_rmsle_score = rmsle(train_filtered.loc[valid_oof.index, TARGET_COL].values, valid_oof.values)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Overall RMSLE across segments: {overall_rmsle_score:.5f}\")\n",
    "if USE_STACKING:\n",
    "    print(f\"Approach: Stacking Ensemble (5 models + ElasticNet meta)\")\n",
    "else:\n",
    "    print(f\"Approach: {'Full data retrain' if USE_FULL_RETRAIN else 'CV fold averaging'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if not USE_STACKING:\n",
    "    feature_importance_summary = (\n",
    "        pd.concat(all_feature_importances, ignore_index=True)\n",
    "        .groupby([\"segment\", \"feature\"], as_index=False)[\"importance\"]\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "oof_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 13. Inference on Test Segments & Blending\n",
    "Combine segment-wise predictions into a single test forecast vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_prediction = train_filtered[TARGET_COL].median()\n",
    "test_predictions_series = test_predictions_series.fillna(fallback_prediction)\n",
    "\n",
    "test_predictions_series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 14. Create Submission File\n",
    "Export the blended predictions in the official submission format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    ID_COL: test_work[ID_COL].values,\n",
    "    TARGET_COL: np.maximum(test_predictions_series.loc[test_work.index].values, 0),\n",
    "})\n",
    "\n",
    "if USE_STACKING:\n",
    "    suffix = \"stacking_ensemble\"\n",
    "else:\n",
    "    suffix = \"full_retrain\" if USE_FULL_RETRAIN else \"cv_avg\"\n",
    "\n",
    "submission_path = OUTPUT_DIR / f\"submission_lightgbm_monotonic_{suffix}.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "if USE_STACKING:\n",
    "    print(f\"Approach: Stacking Ensemble (LightGBM + XGBoost + CatBoost + Ridge + RandomForest)\")\n",
    "else:\n",
    "    print(f\"Approach: {'Full data retrain' if USE_FULL_RETRAIN else 'CV fold averaging'}\")\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_STACKING:\n",
    "    top_features = (\n",
    "        feature_importance_summary.groupby(\"feature\", as_index=False)[\"importance\"].mean()\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .head(20)\n",
    "    )\n",
    "    display(top_features)\n",
    "else:\n",
    "    print(\"Feature importance analysis:\")\n",
    "    print(\"\\nLevel-1 Model Scores:\")\n",
    "    for seg_name, seg_result in segment_results.items():\n",
    "        if \"level1_results\" in seg_result:\n",
    "            print(f\"\\n{seg_name}:\")\n",
    "            for model_name, model_result in seg_result[\"level1_results\"].items():\n",
    "                print(f\"  {model_name}: {model_result['score']:.5f}\")\n",
    "            print(f\"  Meta-Model: {seg_result['meta_score']:.5f}\")\n",
    "            print(f\"  Simple Avg: {seg_result['simple_avg_score']:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 15. Model Comparison & Diagnostics\n",
    "Compare predictions and feature importance between approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nLocal CV RMSLE: {overall_rmsle_score:.5f}\")\n",
    "if USE_STACKING:\n",
    "    print(f\"Training approach: Stacking Ensemble\")\n",
    "    print(f\"  Level-1: LightGBM, XGBoost, CatBoost, ElasticNet, RandomForest\")\n",
    "    print(f\"  Level-2: ElasticNet Regression (meta-model)\")\n",
    "else:\n",
    "    print(f\"Training approach: {'Full data retrain' if USE_FULL_RETRAIN else 'CV fold averaging'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEGMENT SCORES\")\n",
    "print(\"=\"*60)\n",
    "for seg_name, seg_result in segment_results.items():\n",
    "    print(f\"\\n{seg_name}:\")\n",
    "    if USE_STACKING and \"level1_results\" in seg_result:\n",
    "        print(\"  Level-1 Models:\")\n",
    "        for model_name, model_result in seg_result[\"level1_results\"].items():\n",
    "            print(f\"    {model_name}: {model_result['score']:.5f}\")\n",
    "        print(f\"  Level-2 Meta (ElasticNet): {seg_result['meta_score']:.5f}\")\n",
    "        print(f\"  Simple Average: {seg_result['simple_avg_score']:.5f}\")\n",
    "        print(f\"  Meta improvement: {seg_result['simple_avg_score'] - seg_result['meta_score']:.5f}\")\n",
    "    else:\n",
    "        print(f\"  RMSLE: {seg_result['score_mean']:.5f} ± {seg_result['score_std']:.5f}\")\n",
    "        if 'avg_best_iteration' in seg_result:\n",
    "            print(f\"  Avg best iteration: {seg_result['avg_best_iteration']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOOF predictions:\")\n",
    "print(oof_series.describe())\n",
    "print(\"\\nTest predictions:\")\n",
    "print(test_predictions_series.describe())\n",
    "print(\"\\nTrain target:\")\n",
    "print(train_filtered[TARGET_COL].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Experiment Notes\n",
    "\n",
    "**Stacking vs Single Model Comparison:**\n",
    "\n",
    "To compare approaches, run with different settings:\n",
    "1. `USE_STACKING = True` → generates `submission_lightgbm_monotonic_stacking_ensemble.csv`\n",
    "2. `USE_STACKING = False, USE_FULL_RETRAIN = True` → generates `submission_lightgbm_monotonic_full_retrain.csv`\n",
    "3. `USE_STACKING = False, USE_FULL_RETRAIN = False` → generates `submission_lightgbm_monotonic_cv_avg.csv`\n",
    "\n",
    "**Expected differences:**\n",
    "- **Stacking ensemble**: Best performance expected due to model diversity (LightGBM, XGBoost, CatBoost, ElasticNet, RF)\n",
    "- **Full retrain**: May have slightly lower LB score but uses all training data\n",
    "- **CV average**: More robust ensemble, better generalization on unseen data\n",
    "\n",
    "**Stacking benefits** (based on similar competitions):\n",
    "- Combines strengths of tree-based (LightGBM, XGBoost, CatBoost) and linear models (ElasticNet)\n",
    "- RandomForest adds bagging diversity vs boosting methods\n",
    "- ElasticNet meta-model learns optimal weighting with both L1 and L2 regularization\n",
    "- Expected 0.001-0.003 RMSLE improvement over single model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 16. Data Leakage Fix Validation\n",
    "Verify that aggregation features are computed correctly inside CV folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA LEAKAGE FIX VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✓ Aggregation features are now computed inside each CV fold\")\n",
    "print(\"✓ Each fold uses only its training data to build aggregations\")\n",
    "print(\"✓ Validation and test data use aggregations from training fold only\")\n",
    "\n",
    "if USE_STACKING:\n",
    "    print(\"\\n✓ STACKING ENSEMBLE VALIDATION:\")\n",
    "    print(\"  - Level-1 models trained with proper CV (no future data)\")\n",
    "    print(\"  - OOF predictions collected from validation folds only\")\n",
    "    print(\"  - Level-2 meta-model trained on OOF predictions (no leakage)\")\n",
    "    print(\"  - Test predictions from Level-1 averaged across folds\")\n",
    "\n",
    "print(\"\\nExpected impact:\")\n",
    "if USE_STACKING:\n",
    "    print(\"- Stacking should improve over single model by 0.001-0.003 RMSLE\")\n",
    "    print(\"- Model diversity reduces overfitting\")\n",
    "else:\n",
    "    print(\"- Local CV score may INCREASE (worse) - this is expected!\")\n",
    "    print(\"- Public LB score should DECREASE (better) - reduced overfitting\")\n",
    "print(\"- CV-LB gap should be SMALLER - more reliable validation\")\n",
    "\n",
    "print(\"\\nSegment distribution check:\")\n",
    "print(\"Train segments:\", train_filtered[\"is_land_only\"].value_counts().to_dict())\n",
    "print(\"Test segments:\", test_work[\"is_land_only\"].value_counts().to_dict())\n",
    "\n",
    "print(\"\\nSubmission validation:\")\n",
    "print(f\"Total test predictions: {len(test_predictions_series)}\")\n",
    "print(f\"Non-null predictions: {test_predictions_series.notna().sum()}\")\n",
    "print(f\"Negative predictions: {(test_predictions_series < 0).sum()}\")\n",
    "print(f\"NaN predictions: {test_predictions_series.isna().sum()}\")\n",
    "\n",
    "if len(submission_df) > 0:\n",
    "    print(f\"\\nSubmission file check:\")\n",
    "    print(f\"IDs match test: {(submission_df['Id'].values == test_work['Id'].values).all()}\")\n",
    "    print(f\"Price range: {submission_df['TradePrice'].min():.0f} - {submission_df['TradePrice'].max():.0f}\")\n",
    "    print(f\"Mean price: {submission_df['TradePrice'].mean():.0f}\")\n",
    "    print(f\"Median price: {submission_df['TradePrice'].median():.0f}\")\n",
    "\n",
    "if USE_STACKING:\n",
    "    print(f\"\\nStacking configuration:\")\n",
    "    print(f\"Level-1 models: {', '.join(segment_results[list(segment_results.keys())[0]]['level1_model_names'])}\")\n",
    "    print(f\"Level-2 meta-model: ElasticNet Regression\")\n",
    "    print(f\"Cross-validation folds: {N_SPLITS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 17. Stacking Strategy Details\n",
    "\n",
    "### Implementation Overview\n",
    "\n",
    "This notebook now implements a **two-level stacking ensemble** based on successful strategies from similar real estate competitions (House Prices, Zillow Prize).\n",
    "\n",
    "#### Level-1 Models (Base Models)\n",
    "1. **LightGBM** - Fast GBDT, handles missing values, categorical features natively\n",
    "2. **XGBoost** - Alternative GBDT implementation with different error patterns\n",
    "3. **CatBoost** - Specialized for high-cardinality categorical features (stations, districts)\n",
    "4. **ElasticNet** - Linear model with both L1 and L2 regularization, captures linear trends and performs feature selection\n",
    "5. **RandomForest** - Bagging approach (vs boosting), robust to overfitting\n",
    "\n",
    "Each model trained with 5-fold CV to generate Out-Of-Fold (OOF) predictions.\n",
    "\n",
    "#### Level-2 Model (Meta-Model)\n",
    "- **ElasticNet Regression** with combined L1 and L2 regularization\n",
    "- Trained on OOF predictions from Level-1 models\n",
    "- Learns optimal weighting with feature selection capabilities (L1) and coefficient shrinkage (L2)\n",
    "- Balanced regularization (l1_ratio=0.5) prevents overfitting while maintaining model interpretability\n",
    "- Proven effective in similar competitions for meta-learning\n",
    "\n",
    "#### Key Benefits\n",
    "- **Model diversity**: Combines tree-based and linear approaches\n",
    "- **Error compensation**: Different models make different mistakes\n",
    "- **Robust predictions**: Less sensitive to data anomalies\n",
    "- **Feature selection**: ElasticNet's L1 component can automatically select the most useful base model predictions\n",
    "- **Historical success**: Similar strategies achieved top ranks in housing competitions\n",
    "\n",
    "#### Expected Performance\n",
    "Based on similar competitions:\n",
    "- 0.001-0.003 RMSLE improvement over single best model\n",
    "- More stable predictions across different market segments\n",
    "- Better handling of high-value properties (outliers)\n",
    "\n",
    "### Configuration\n",
    "\n",
    "Set `USE_STACKING = True` to enable stacking ensemble.\n",
    "Set `USE_STACKING = False` to use single LightGBM model (baseline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### How to Use\n",
    "\n",
    "**Step 1: Install required packages** (if needed)\n",
    "```python\n",
    "%pip install -q lightgbm xgboost catboost\n",
    "```\n",
    "\n",
    "**Step 2: Choose your strategy**\n",
    "- For stacking ensemble: Set `USE_STACKING = True` (recommended)\n",
    "- For single LightGBM: Set `USE_STACKING = False`\n",
    "\n",
    "**Step 3: Run all cells**\n",
    "The notebook will automatically:\n",
    "1. Train 5 diverse models (if stacking enabled)\n",
    "2. Collect OOF predictions from each model\n",
    "3. Train ElasticNet meta-model on OOF predictions\n",
    "4. Generate final predictions for test set\n",
    "5. Save submission file\n",
    "\n",
    "**Step 4: Compare results**\n",
    "Submit both versions to Kaggle and compare:\n",
    "- `submission_lightgbm_monotonic_stacking_ensemble.csv` (stacking)\n",
    "- `submission_lightgbm_monotonic_full_retrain.csv` (single model)\n",
    "\n",
    "### Training Time Expectations\n",
    "\n",
    "**Single LightGBM**: ~5-10 minutes\n",
    "**Stacking Ensemble**: ~30-45 minutes (5 models × 2 segments × 5 folds)\n",
    "\n",
    "Stacking takes longer but provides better generalization and stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
