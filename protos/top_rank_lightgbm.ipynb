{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65a586a",
   "metadata": {},
   "source": [
    "# E-Style Real Estate Price Prediction\n",
    "Kaggle competition notebook featuring LightGBM with monotonic constraints, type-specific modeling, and RMSLE optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb03b1f",
   "metadata": {},
   "source": [
    "## 1. Setup Libraries & Configuration\n",
    "Import core libraries, fix random seeds, and define helpers for RMSLE tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9343145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If LightGBM is missing in your environment, uncomment the next line.\n",
    "# %pip install -q lightgbm\n",
    "\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "SEED = 2025\n",
    "N_SPLITS = 5\n",
    "TARGET_COL = \"TradePrice\"\n",
    "ID_COL = \"Id\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\")\n",
    "\n",
    "\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    \"\"\"Fix all relevant random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def rmsle(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Compute RMSLE while protecting against negative predictions.\"\"\"\n",
    "    y_true = np.clip(y_true, a_min=0, a_max=None)\n",
    "    y_pred = np.clip(y_pred, a_min=0, a_max=None)\n",
    "    return math.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def memory_info(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Return a human-readable memory usage string for quick diagnostics.\"\"\"\n",
    "    usage_mb = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    return f\"{usage_mb:,.2f} MB\"\n",
    "\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f20ca",
   "metadata": {},
   "source": [
    "## 2. Load Datasets\n",
    "Read raw CSV files with consistent schema handling and sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eff91237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (300000, 35), memory: 298.86 MB\n",
      "Test shape:  (600000, 34), memory: 593.08 MB\n",
      "Sample submission shape: (600000, 2)\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path.cwd().resolve()\n",
    "DATA_DIR = BASE_DIR.parent / \"input\" / \"estyle-community-competition-2025\"\n",
    "OUTPUT_DIR = BASE_DIR.parent / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_path = DATA_DIR / \"train.csv\"\n",
    "test_path = DATA_DIR / \"test.csv\"\n",
    "sample_submission_path = DATA_DIR / \"sample_submission.csv\"\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing train data at {train_path}\")\n",
    "\n",
    "train_df = pd.read_csv(train_path, low_memory=False)\n",
    "test_df = pd.read_csv(test_path, low_memory=False)\n",
    "sample_submission = pd.read_csv(sample_submission_path, low_memory=False)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, memory: {memory_info(train_df)}\")\n",
    "print(f\"Test shape:  {test_df.shape}, memory: {memory_info(test_df)}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd42170",
   "metadata": {},
   "source": [
    "## 3. Basic Cleaning & Type Casting\n",
    "Align numerical dtypes and ensure train/test columns match before feature work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94bacc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-alignment train dtypes summary:\n",
      " object     17\n",
      "float64     8\n",
      "int64       6\n",
      "Int8        4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Strip whitespace from column names to avoid subtle mismatches.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_boolean_columns(df: pd.DataFrame, bool_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Ensure boolean indicator columns are stored as integers for LightGBM.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in bool_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"Int8\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def align_train_test(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Run basic normalization and confirm schema alignment.\"\"\"\n",
    "    bool_columns = [\n",
    "        \"AreaIsGreaterFlag\",\n",
    "        \"FrontageIsGreaterFlag\",\n",
    "        \"TotalFloorAreaIsGreaterFlag\",\n",
    "        \"PrewarBuilding\",\n",
    "    ]\n",
    "    train_clean = cast_boolean_columns(standardize_columns(train), bool_columns)\n",
    "    test_clean = cast_boolean_columns(standardize_columns(test), bool_columns)\n",
    "\n",
    "    missing_in_test = sorted(set(train_clean.columns) - set(test_clean.columns) - {TARGET_COL})\n",
    "    if missing_in_test:\n",
    "        print(\"Columns present in train but absent in test:\", missing_in_test)\n",
    "    return train_clean, test_clean\n",
    "\n",
    "\n",
    "train_df, test_df = align_train_test(train_df, test_df)\n",
    "print(\"Post-alignment train dtypes summary:\\n\", train_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665b3c3",
   "metadata": {},
   "source": [
    "## 4. Missing Value Handling & Flag Features\n",
    "Impute categorical gaps with `'unknown'` and add binary indicators for all missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51ab07a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indicators added for 22 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_missing_ratio</th>\n",
       "      <th>test_missing_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Remarks</th>\n",
       "      <td>0.9341</td>\n",
       "      <td>0.9345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Renovation</th>\n",
       "      <td>0.7386</td>\n",
       "      <td>0.7380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FloorPlan</th>\n",
       "      <td>0.7226</td>\n",
       "      <td>0.7218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Purpose</th>\n",
       "      <td>0.6520</td>\n",
       "      <td>0.6515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalFloorArea</th>\n",
       "      <td>0.6219</td>\n",
       "      <td>0.6233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frontage</th>\n",
       "      <td>0.3790</td>\n",
       "      <td>0.3801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Use</th>\n",
       "      <td>0.3572</td>\n",
       "      <td>0.3583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BuildingYear</th>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.3483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Structure</th>\n",
       "      <td>0.3427</td>\n",
       "      <td>0.3441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breadth</th>\n",
       "      <td>0.3322</td>\n",
       "      <td>0.3328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classification</th>\n",
       "      <td>0.3268</td>\n",
       "      <td>0.3276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direction</th>\n",
       "      <td>0.3174</td>\n",
       "      <td>0.3179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                train_missing_ratio  test_missing_ratio\n",
       "Remarks                      0.9341              0.9345\n",
       "Renovation                   0.7386              0.7380\n",
       "FloorPlan                    0.7226              0.7218\n",
       "Purpose                      0.6520              0.6515\n",
       "TotalFloorArea               0.6219              0.6233\n",
       "Frontage                     0.3790              0.3801\n",
       "Use                          0.3572              0.3583\n",
       "BuildingYear                 0.3478              0.3483\n",
       "Structure                    0.3427              0.3441\n",
       "Breadth                      0.3322              0.3328\n",
       "Classification               0.3268              0.3276\n",
       "Direction                    0.3174              0.3179"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_work = train_df.copy()\n",
    "test_work = test_df.copy()\n",
    "\n",
    "missing_summary = (\n",
    "    pd.DataFrame({\n",
    "        \"train_missing_ratio\": train_work.isna().mean(),\n",
    "        \"test_missing_ratio\": test_work.isna().mean(),\n",
    "    })\n",
    "    .sort_values(\"train_missing_ratio\", ascending=False)\n",
    ")\n",
    "\n",
    "missing_columns = [\n",
    "    col\n",
    "    for col in missing_summary.index\n",
    "    if (train_work[col].isna().any() if col in train_work.columns else False)\n",
    "    or (test_work[col].isna().any() if col in test_work.columns else False)\n",
    "]\n",
    "\n",
    "categorical_columns = sorted(\n",
    "    set(train_work.select_dtypes(include=[\"object\"]).columns)\n",
    "    | set(test_work.select_dtypes(include=[\"object\"]).columns)\n",
    ")\n",
    "\n",
    "def add_missing_indicators(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_missing_flag\"] = df[col].isna().astype(\"int8\")\n",
    "\n",
    "def fill_categorical_unknown(train_df: pd.DataFrame, test_df: pd.DataFrame, cat_cols: List[str]) -> None:\n",
    "    for col in cat_cols:\n",
    "        if col in train_df.columns:\n",
    "            train_df[col] = train_df[col].fillna(\"unknown\")\n",
    "        if col in test_df.columns:\n",
    "            test_df[col] = test_df[col].fillna(\"unknown\")\n",
    "\n",
    "def fill_numeric_with_median(train_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    numeric_cols = sorted(set(train_df.select_dtypes(include=[np.number]).columns))\n",
    "    for col in numeric_cols:\n",
    "        if col == TARGET_COL:\n",
    "            continue\n",
    "        median_value = train_df[col].median()\n",
    "        if np.isnan(median_value):\n",
    "            median_value = 0.0\n",
    "        train_df[col] = train_df[col].fillna(median_value)\n",
    "        if col in test_df.columns:\n",
    "            test_df[col] = test_df[col].fillna(median_value)\n",
    "\n",
    "add_missing_indicators(train_work, missing_columns)\n",
    "add_missing_indicators(test_work, missing_columns)\n",
    "fill_categorical_unknown(train_work, test_work, categorical_columns)\n",
    "# fill_numeric_with_median(train_work, test_work)\n",
    "\n",
    "print(\"Missing indicators added for\", len(missing_columns), \"columns.\")\n",
    "missing_summary.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d281ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Type', 'Region', 'MunicipalityCode', 'Prefecture',\n",
       "       'Municipality', 'DistrictName', 'NearestStation',\n",
       "       'TimeToNearestStation', 'MinTimeToNearestStation',\n",
       "       'MaxTimeToNearestStation', 'TradePrice', 'FloorPlan', 'Area',\n",
       "       'AreaIsGreaterFlag', 'LandShape', 'Frontage', 'FrontageIsGreaterFlag',\n",
       "       'TotalFloorArea', 'TotalFloorAreaIsGreaterFlag', 'BuildingYear',\n",
       "       'PrewarBuilding', 'Structure', 'Use', 'Purpose', 'Direction',\n",
       "       'Classification', 'Breadth', 'CityPlanning', 'CoverageRatio',\n",
       "       'FloorAreaRatio', 'Year', 'Quarter', 'Renovation', 'Remarks',\n",
       "       'Remarks_missing_flag', 'Renovation_missing_flag',\n",
       "       'FloorPlan_missing_flag', 'Purpose_missing_flag',\n",
       "       'TotalFloorArea_missing_flag', 'Frontage_missing_flag',\n",
       "       'Use_missing_flag', 'BuildingYear_missing_flag',\n",
       "       'Structure_missing_flag', 'Breadth_missing_flag',\n",
       "       'Classification_missing_flag', 'Direction_missing_flag',\n",
       "       'LandShape_missing_flag', 'Region_missing_flag',\n",
       "       'CoverageRatio_missing_flag', 'FloorAreaRatio_missing_flag',\n",
       "       'MaxTimeToNearestStation_missing_flag',\n",
       "       'MinTimeToNearestStation_missing_flag',\n",
       "       'TimeToNearestStation_missing_flag', 'CityPlanning_missing_flag',\n",
       "       'NearestStation_missing_flag', 'DistrictName_missing_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_work.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd70148",
   "metadata": {},
   "source": [
    "## 5. High Missing Columns Pruning\n",
    "Drop highly sparse fields while keeping their missingness indicators for signal preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "207be4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1 sparse columns: ['Remarks']\n"
     ]
    }
   ],
   "source": [
    "HIGH_MISSING_THRESHOLD = 0.85\n",
    "\n",
    "high_missing_cols = [\n",
    "    col\n",
    "    for col, ratios in missing_summary.iterrows()\n",
    "    if ratios[\"train_missing_ratio\"] >= HIGH_MISSING_THRESHOLD\n",
    "    and col not in {TARGET_COL}\n",
    "]\n",
    "\n",
    "train_work.drop(columns=[col for col in high_missing_cols if col in train_work.columns], inplace=True)\n",
    "test_work.drop(columns=[col for col in high_missing_cols if col in test_work.columns], inplace=True)\n",
    "\n",
    "print(f\"Dropped {len(high_missing_cols)} sparse columns: {high_missing_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bdfb60",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection & Removal\n",
    "Remove extreme target values using log-scale quantile clipping to stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98f364df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier trimming retained 297074 of 300000 rows (99.02%).\n",
      "Filtered train shape: (297074, 56)\n"
     ]
    }
   ],
   "source": [
    "def trim_outliers_log(df: pd.DataFrame, target_col: str, lower_quantile: float = 0.005, upper_quantile: float = 0.995) -> pd.DataFrame:\n",
    "    \"\"\"Remove rows with extreme targets on the log scale.\"\"\"\n",
    "    log_target = np.log1p(df[target_col])\n",
    "    low, high = log_target.quantile([lower_quantile, upper_quantile])\n",
    "    mask = log_target.between(low, high)\n",
    "    trimmed = df.loc[mask].copy()\n",
    "    print(\n",
    "        f\"Outlier trimming retained {mask.sum()} of {len(mask)} rows \"\n",
    "        f\"({mask.sum() / len(mask):.2%}).\"\n",
    "    )\n",
    "    return trimmed\n",
    "\n",
    "\n",
    "train_filtered = trim_outliers_log(train_work, TARGET_COL)\n",
    "print(\"Filtered train shape:\", train_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69f4e9",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering: DistrictName × BuildingYear Aggregations\n",
    "Capture localized pricing signals with smoothed mean/median targets by district and construction year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53a4dd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district_buildyear_price_mean</th>\n",
       "      <th>district_buildyear_price_median</th>\n",
       "      <th>district_buildyear_price_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>297,074.0000</td>\n",
       "      <td>297,074.0000</td>\n",
       "      <td>297,074.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>42,678,586.4915</td>\n",
       "      <td>30,193,923.4092</td>\n",
       "      <td>16.5103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10,758,608.1179</td>\n",
       "      <td>41,727,357.1724</td>\n",
       "      <td>35.6271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8,522,413.0007</td>\n",
       "      <td>11,000.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40,049,274.2073</td>\n",
       "      <td>14,000,000.0000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>42,169,896.6779</td>\n",
       "      <td>25,000,000.0000</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43,026,739.1637</td>\n",
       "      <td>37,000,000.0000</td>\n",
       "      <td>17.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>579,902,822.3681</td>\n",
       "      <td>6,019,500,000.0000</td>\n",
       "      <td>433.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       district_buildyear_price_mean  district_buildyear_price_median  \\\n",
       "count                   297,074.0000                     297,074.0000   \n",
       "mean                 42,678,586.4915                  30,193,923.4092   \n",
       "std                  10,758,608.1179                  41,727,357.1724   \n",
       "min                   8,522,413.0007                      11,000.0000   \n",
       "25%                  40,049,274.2073                  14,000,000.0000   \n",
       "50%                  42,169,896.6779                  25,000,000.0000   \n",
       "75%                  43,026,739.1637                  37,000,000.0000   \n",
       "max                 579,902,822.3681               6,019,500,000.0000   \n",
       "\n",
       "       district_buildyear_price_count  \n",
       "count                    297,074.0000  \n",
       "mean                          16.5103  \n",
       "std                           35.6271  \n",
       "min                            1.0000  \n",
       "25%                            2.0000  \n",
       "50%                            6.0000  \n",
       "75%                           17.0000  \n",
       "max                          433.0000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_district_buildyear_agg(train_source: pd.DataFrame) -> Dict[str, Dict[Tuple[str, int], float]]:\n",
    "    helper = train_source[[ID_COL, \"DistrictName\", \"BuildingYear\", TARGET_COL]].copy()\n",
    "    helper[\"DistrictName\"] = helper[\"DistrictName\"].fillna(\"unknown\")\n",
    "    helper[\"BuildingYearGroup\"] = helper[\"BuildingYear\"].fillna(-1).round().astype(int)\n",
    "\n",
    "    grouped = helper.groupby([\"DistrictName\", \"BuildingYearGroup\"])[TARGET_COL].agg([\"mean\", \"median\", \"count\"])\n",
    "    global_mean = helper[TARGET_COL].mean()\n",
    "    grouped[\"smoothed_mean\"] = (\n",
    "        (grouped[\"mean\"] * grouped[\"count\"]) + (global_mean * 50)\n",
    "    ) / (grouped[\"count\"] + 50)\n",
    "\n",
    "    return {\n",
    "    \"smoothed_mean\": grouped[\"smoothed_mean\"].to_dict(),\n",
    "    \"median\": grouped[\"median\"].to_dict(),\n",
    "    \"count\": grouped[\"count\"].to_dict(),\n",
    "    \"global_mean\": global_mean,\n",
    "    \"global_median\": helper[TARGET_COL].median(),\n",
    "}\n",
    "\n",
    "\n",
    "def apply_agg_features(\n",
    "    df: pd.DataFrame,\n",
    "    agg_lookup: Dict[str, Dict[Tuple[str, int], float]],\n",
    "    district_source: pd.Series,\n",
    "    building_year_source: pd.Series,\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    district_values = df[ID_COL].map(district_source).fillna(\"unknown\")\n",
    "    building_year_values = df[ID_COL].map(building_year_source).fillna(-1).round().astype(int)\n",
    "    keys = list(zip(district_values, building_year_values))\n",
    "\n",
    "    df[\"district_buildyear_price_mean\"] = [\n",
    "        agg_lookup[\"smoothed_mean\"].get(key, agg_lookup[\"global_mean\"])\n",
    "        for key in keys\n",
    "    ]\n",
    "    df[\"district_buildyear_price_median\"] = [\n",
    "        agg_lookup[\"median\"].get(key, agg_lookup[\"global_median\"])\n",
    "        for key in keys\n",
    "    ]\n",
    "    df[\"district_buildyear_price_count\"] = [\n",
    "        agg_lookup[\"count\"].get(key, 0.0)\n",
    "        for key in keys\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "\n",
    "agg_lookup = build_district_buildyear_agg(train_df)\n",
    "train_filtered = apply_agg_features(\n",
    "    train_filtered,\n",
    "    agg_lookup,\n",
    "    district_source=train_df.set_index(ID_COL)[\"DistrictName\"],\n",
    "    building_year_source=train_df.set_index(ID_COL)[\"BuildingYear\"],\n",
    ")\n",
    "test_work = apply_agg_features(\n",
    "    test_work,\n",
    "    agg_lookup,\n",
    "    district_source=test_df.set_index(ID_COL)[\"DistrictName\"],\n",
    "    building_year_source=test_df.set_index(ID_COL)[\"BuildingYear\"],\n",
    ")\n",
    "\n",
    "train_filtered[[\n",
    "    \"district_buildyear_price_mean\",\n",
    "    \"district_buildyear_price_median\",\n",
    "    \"district_buildyear_price_count\",\n",
    "]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54df8e",
   "metadata": {},
   "source": [
    "## 8. Categorical Encoding & Label Preparation\n",
    "Convert remaining object columns to categorical dtype and define target transformations for RMSLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bbe6025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns prepared: 16\n"
     ]
    }
   ],
   "source": [
    "def add_domain_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if {\"Year\", \"BuildingYear\"}.issubset(df.columns):\n",
    "        df[\"BuildingAge\"] = (df[\"Year\"] - df[\"BuildingYear\"]).clip(lower=0)\n",
    "        flag_col = \"BuildingYear_missing_flag\"\n",
    "        if flag_col in df.columns:\n",
    "            df.loc[df[flag_col] == 1, \"BuildingAge\"] = np.nan\n",
    "    if \"Area\" in df.columns:\n",
    "        df[\"Area_log\"] = np.log1p(df[\"Area\"])\n",
    "    if \"TotalFloorArea\" in df.columns:\n",
    "        df[\"TotalFloorArea_log\"] = np.log1p(df[\"TotalFloorArea\"])\n",
    "        df[\"FloorArea_to_Area\"] = df[\"TotalFloorArea\"] / (df[\"Area\"] + 1e-3)\n",
    "    if {\"Frontage\", \"Area\"}.issubset(df.columns):\n",
    "        df[\"Frontage_to_sqrtArea\"] = df[\"Frontage\"] / (np.sqrt(df[\"Area\"]) + 1e-3)\n",
    "    if {\"MaxTimeToNearestStation\", \"MinTimeToNearestStation\"}.issubset(df.columns):\n",
    "        df[\"StationTimeRange\"] = df[\"MaxTimeToNearestStation\"] - df[\"MinTimeToNearestStation\"]\n",
    "    df[\"district_buildyear_price_count_log\"] = np.log1p(df.get(\"district_buildyear_price_count\", 0.0))\n",
    "    return df\n",
    "\n",
    "\n",
    "train_filtered = add_domain_features(train_filtered)\n",
    "test_work = add_domain_features(test_work)\n",
    "\n",
    "fill_numeric_with_median(train_filtered, test_work)\n",
    "\n",
    "categorical_cols_final = sorted(\n",
    "    set(train_filtered.select_dtypes(include=[\"object\"]).columns)\n",
    "    | set(test_work.select_dtypes(include=[\"object\"]).columns)\n",
    ")\n",
    "for col in categorical_cols_final:\n",
    "    if col in train_filtered.columns:\n",
    "        train_filtered[col] = train_filtered[col].astype(\"category\")\n",
    "    if col in test_work.columns:\n",
    "        test_work[col] = test_work[col].astype(\"category\")\n",
    "\n",
    "train_target = train_filtered[TARGET_COL].copy()\n",
    "print(\"Categorical columns prepared:\", len(categorical_cols_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb538da",
   "metadata": {},
   "source": [
    "## 9. Segment Datasets by Property Type\n",
    "Split samples into `land only` and `with building` segments to train specialized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "619f35b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_land_only</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              share\n",
       "is_land_only       \n",
       "1            0.7109\n",
       "0            0.2891"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAND_KEYWORDS = [\"land\", \"土地\", \"宅地\", \"lot\", \"residential land\", \"commercial land\"]\n",
    "\n",
    "\n",
    "def detect_land_only(type_series: pd.Series) -> pd.Series:\n",
    "    type_str = type_series.astype(str).str.lower()\n",
    "    pattern = \"|\".join(LAND_KEYWORDS)\n",
    "    land_mask = type_str.str.contains(pattern, case=False, na=False)\n",
    "    return land_mask\n",
    "\n",
    "\n",
    "train_filtered[\"is_land_only\"] = detect_land_only(train_filtered[\"Type\"]).astype(\"int8\")\n",
    "test_work[\"is_land_only\"] = detect_land_only(test_work[\"Type\"]).astype(\"int8\")\n",
    "\n",
    "train_filtered[\"Type\"] = train_filtered[\"Type\"].cat.remove_unused_categories()\n",
    "\n",
    "train_filtered[\"is_land_only\"].value_counts(normalize=True).rename(\"share\").to_frame(\"share\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2f760",
   "metadata": {},
   "source": [
    "## 10. LightGBM Monotonic Constraint Definitions\n",
    "Enforce domain knowledge (e.g., larger area ⇒ higher price) via monotone constraints per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "207b4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONOTONIC_FEATURE_MAP = {\n",
    "    \"Area\": 1,\n",
    "    \"Area_log\": 1,\n",
    "    \"TotalFloorArea\": 1,\n",
    "    \"TotalFloorArea_log\": 1,\n",
    "    \"FloorArea_to_Area\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "def build_monotonic_constraints(feature_names: List[str]) -> str:\n",
    "    \"\"\"Return LightGBM-compatible monotone constraint string.\"\"\"\n",
    "    constraints = [MONOTONIC_FEATURE_MAP.get(name, 0) for name in feature_names]\n",
    "    return \"(\" + \",\".join(str(int(val)) for val in constraints) + \")\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef54552",
   "metadata": {},
   "source": [
    "## 11. K-Fold Cross-Validation Workflow\n",
    "Train LightGBM models per segment with RMSLE-focused validation and constraint-aware parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02954412",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_FEATURES = {TARGET_COL, ID_COL, \"district_buildyear_price_count\", \"is_land_only\"}\n",
    "\n",
    "LIGHTGBM_PARAMS_BASE = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"n_estimators\": 5000,\n",
    "    \"num_leaves\": 128,\n",
    "    \"max_depth\": -1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"min_child_samples\": 50,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "\n",
    "def get_feature_columns(df: pd.DataFrame) -> List[str]:\n",
    "    return [col for col in df.columns if col not in EXCLUDE_FEATURES]\n",
    "\n",
    "\n",
    "def run_segment_cv(\n",
    "    train_segment: pd.DataFrame,\n",
    "    test_segment: pd.DataFrame,\n",
    "    segment_name: str,\n",
    "    seed: int = SEED,\n",
    "    n_splits: int = N_SPLITS,\n",
    "    retrain_on_full: bool = True,\n",
    ") -> Dict[str, object]:\n",
    "    features = get_feature_columns(train_segment)\n",
    "    cat_features = [col for col in features if str(train_segment[col].dtype) == \"category\"]\n",
    "    monotone_constraints = build_monotonic_constraints(features)\n",
    "\n",
    "    X = train_segment[features]\n",
    "    y = np.log1p(train_segment[TARGET_COL].values)\n",
    "    X_test = test_segment[features]\n",
    "\n",
    "    oof_pred = np.zeros(len(train_segment))\n",
    "    test_pred = np.zeros(len(test_segment))\n",
    "    fold_scores = []\n",
    "    feature_importances = []\n",
    "    best_iterations = []\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), start=1):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "\n",
    "        params = LIGHTGBM_PARAMS_BASE.copy()\n",
    "        params.update({\"monotone_constraints\": monotone_constraints, \"random_state\": seed + fold})\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\",\n",
    "            categorical_feature=cat_features,\n",
    "            callbacks=[lgb.early_stopping(200), lgb.log_evaluation(200)],\n",
    "        )\n",
    "\n",
    "        best_iterations.append(model.best_iteration_)\n",
    "        \n",
    "        val_pred = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "        oof_pred[valid_idx] = np.maximum(np.expm1(val_pred), 0)\n",
    "\n",
    "        if not retrain_on_full:\n",
    "            test_pred += np.maximum(\n",
    "                np.expm1(model.predict(X_test, num_iteration=model.best_iteration_)),\n",
    "                0,\n",
    "            ) / n_splits\n",
    "\n",
    "        fold_score = rmsle(train_segment.iloc[valid_idx][TARGET_COL].values, oof_pred[valid_idx])\n",
    "        fold_scores.append(fold_score)\n",
    "\n",
    "        fold_importance = pd.DataFrame({\n",
    "            \"feature\": features,\n",
    "            \"importance\": model.booster_.feature_importance(importance_type=\"gain\"),\n",
    "            \"fold\": fold,\n",
    "            \"segment\": segment_name,\n",
    "        })\n",
    "        feature_importances.append(fold_importance)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # Retrain on full data if requested\n",
    "    if retrain_on_full:\n",
    "        print(f\"  Retraining {segment_name} on full training data...\")\n",
    "        avg_best_iteration = int(np.mean(best_iterations))\n",
    "        \n",
    "        params_full = LIGHTGBM_PARAMS_BASE.copy()\n",
    "        params_full.update({\n",
    "            \"monotone_constraints\": monotone_constraints,\n",
    "            \"random_state\": seed,\n",
    "            \"n_estimators\": avg_best_iteration + 50,\n",
    "        })\n",
    "        \n",
    "        final_model = lgb.LGBMRegressor(**params_full)\n",
    "        final_model.fit(\n",
    "            X, y,\n",
    "            categorical_feature=cat_features,\n",
    "            callbacks=[lgb.log_evaluation(200)],\n",
    "        )\n",
    "        \n",
    "        test_pred = np.maximum(\n",
    "            np.expm1(final_model.predict(X_test)),\n",
    "            0,\n",
    "        )\n",
    "\n",
    "    result = {\n",
    "        \"oof\": pd.Series(oof_pred, index=train_segment.index, name=f\"oof_{segment_name}\"),\n",
    "        \"test_pred\": pd.Series(test_pred, index=test_segment.index, name=f\"pred_{segment_name}\"),\n",
    "        \"score_mean\": np.mean(fold_scores),\n",
    "        \"score_std\": np.std(fold_scores),\n",
    "        \"feature_importances\": pd.concat(feature_importances, ignore_index=True),\n",
    "        \"avg_best_iteration\": int(np.mean(best_iterations)),\n",
    "    }\n",
    "    print(f\"Segment {segment_name}: RMSLE {result['score_mean']:.5f} ± {result['score_std']:.5f}\")\n",
    "    if retrain_on_full:\n",
    "        print(f\"  Avg best iteration: {result['avg_best_iteration']}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67472e21",
   "metadata": {},
   "source": [
    "## 12. Train Segment Models on Full Data\n",
    "Execute segmented cross-validation, gather OOF predictions, and summarize feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64778c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11716\n",
      "[LightGBM] [Info] Number of data points in the train set: 168942, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 16.867719\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's rmse: 0.438631\n",
      "[400]\tvalid_0's rmse: 0.42783\n",
      "[600]\tvalid_0's rmse: 0.425909\n",
      "[800]\tvalid_0's rmse: 0.425459\n",
      "[1000]\tvalid_0's rmse: 0.425255\n",
      "Early stopping, best iteration is:\n",
      "[994]\tvalid_0's rmse: 0.42523\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11720\n",
      "[LightGBM] [Info] Number of data points in the train set: 168942, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 16.865635\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's rmse: 0.440818\n",
      "[400]\tvalid_0's rmse: 0.431165\n",
      "[600]\tvalid_0's rmse: 0.429378\n",
      "[800]\tvalid_0's rmse: 0.428754\n",
      "[1000]\tvalid_0's rmse: 0.428899\n",
      "Early stopping, best iteration is:\n",
      "[811]\tvalid_0's rmse: 0.428736\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11734\n",
      "[LightGBM] [Info] Number of data points in the train set: 168942, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 16.864397\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's rmse: 0.439497\n",
      "[400]\tvalid_0's rmse: 0.428869\n",
      "[600]\tvalid_0's rmse: 0.426555\n",
      "[800]\tvalid_0's rmse: 0.425889\n",
      "[1000]\tvalid_0's rmse: 0.425814\n",
      "[1200]\tvalid_0's rmse: 0.425924\n",
      "Early stopping, best iteration is:\n",
      "[1050]\tvalid_0's rmse: 0.42572\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11695\n",
      "[LightGBM] [Info] Number of data points in the train set: 168943, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 16.866409\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's rmse: 0.436761\n",
      "[400]\tvalid_0's rmse: 0.425752\n",
      "[600]\tvalid_0's rmse: 0.423324\n",
      "[800]\tvalid_0's rmse: 0.422874\n",
      "Early stopping, best iteration is:\n",
      "[795]\tvalid_0's rmse: 0.422822\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11694\n",
      "[LightGBM] [Info] Number of data points in the train set: 168943, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 16.865729\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's rmse: 0.435925\n",
      "[400]\tvalid_0's rmse: 0.424568\n",
      "[600]\tvalid_0's rmse: 0.422721\n",
      "[800]\tvalid_0's rmse: 0.422263\n",
      "[1000]\tvalid_0's rmse: 0.422232\n",
      "Early stopping, best iteration is:\n",
      "[997]\tvalid_0's rmse: 0.42221\n",
      "Segment land_only: RMSLE 0.42494 ± 0.00233\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5214\n",
      "[LightGBM] [Info] Number of data points in the train set: 68716, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 16.828634\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's rmse: 0.280864\n",
      "[400]\tvalid_0's rmse: 0.277416\n",
      "[600]\tvalid_0's rmse: 0.278188\n",
      "Early stopping, best iteration is:\n",
      "[408]\tvalid_0's rmse: 0.277408\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5219\n",
      "[LightGBM] [Info] Number of data points in the train set: 68717, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 16.826733\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's rmse: 0.27994\n",
      "[400]\tvalid_0's rmse: 0.276549\n",
      "[600]\tvalid_0's rmse: 0.277176\n",
      "Early stopping, best iteration is:\n",
      "[430]\tvalid_0's rmse: 0.276505\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5232\n",
      "[LightGBM] [Info] Number of data points in the train set: 68717, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 16.825144\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's rmse: 0.272464\n",
      "[400]\tvalid_0's rmse: 0.271064\n",
      "Early stopping, best iteration is:\n",
      "[299]\tvalid_0's rmse: 0.27088\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5190\n",
      "[LightGBM] [Info] Number of data points in the train set: 68717, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 16.827470\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    }
   ],
   "source": [
    "# Choose between two approaches:\n",
    "# - retrain_on_full=True: Retrain on all data after CV (recommended, uses more data)\n",
    "# - retrain_on_full=False: Average predictions from CV folds (more robust to overfitting)\n",
    "USE_FULL_RETRAIN = True\n",
    "\n",
    "segment_mapping = {1: \"land_only\", 0: \"with_building\"}\n",
    "segment_results = {}\n",
    "all_feature_importances = []\n",
    "\n",
    "oof_series = pd.Series(index=train_filtered.index, dtype=float)\n",
    "test_predictions_series = pd.Series(index=test_work.index, dtype=float)\n",
    "\n",
    "for segment_value, segment_name in segment_mapping.items():\n",
    "    train_segment = train_filtered[train_filtered[\"is_land_only\"] == segment_value].copy()\n",
    "    test_segment = test_work[test_work[\"is_land_only\"] == segment_value].copy()\n",
    "\n",
    "    if train_segment.empty:\n",
    "        print(f\"Segment {segment_name} has no training records; skipping.\")\n",
    "        continue\n",
    "\n",
    "    if test_segment.empty:\n",
    "        print(f\"Segment {segment_name} has no test records; predictions will remain NaN.\")\n",
    "\n",
    "    result = run_segment_cv(train_segment, test_segment, segment_name, retrain_on_full=USE_FULL_RETRAIN)\n",
    "    segment_results[segment_name] = result\n",
    "\n",
    "    oof_series.loc[train_segment.index] = result[\"oof\"]\n",
    "    if not test_segment.empty:\n",
    "        test_predictions_series.loc[test_segment.index] = result[\"test_pred\"]\n",
    "\n",
    "    all_feature_importances.append(result[\"feature_importances\"])\n",
    "\n",
    "valid_oof = oof_series.dropna()\n",
    "overall_rmsle_score = rmsle(train_filtered.loc[valid_oof.index, TARGET_COL].values, valid_oof.values)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Overall RMSLE across segments: {overall_rmsle_score:.5f}\")\n",
    "print(f\"Approach: {'Full data retrain' if USE_FULL_RETRAIN else 'CV fold averaging'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "feature_importance_summary = (\n",
    "    pd.concat(all_feature_importances, ignore_index=True)\n",
    "    .groupby([\"segment\", \"feature\"], as_index=False)[\"importance\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "oof_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27268332",
   "metadata": {},
   "source": [
    "## 13. Inference on Test Segments & Blending\n",
    "Combine segment-wise predictions into a single test forecast vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027411ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         600,000.0000\n",
       "mean       32,205,097.8513\n",
       "std        38,794,397.0271\n",
       "min           161,661.7147\n",
       "25%        14,175,259.3401\n",
       "50%        25,155,741.6207\n",
       "75%        37,617,874.9476\n",
       "max     2,856,886,828.2736\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallback_prediction = train_filtered[TARGET_COL].median()\n",
    "test_predictions_series = test_predictions_series.fillna(fallback_prediction)\n",
    "\n",
    "test_predictions_series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3ab3c",
   "metadata": {},
   "source": [
    "## 14. Create Submission File\n",
    "Export the blended predictions in the official submission format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to /Users/estyle-155/Documents/kaggle/kaggle_estyle/output/submission_lightgbm_monotonic.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>TradePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300001</td>\n",
       "      <td>18,366,669.9948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300002</td>\n",
       "      <td>96,312,334.3856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300003</td>\n",
       "      <td>260,533,802.7713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300004</td>\n",
       "      <td>9,878,918.0471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300005</td>\n",
       "      <td>14,471,425.5912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id       TradePrice\n",
       "0  300001  18,366,669.9948\n",
       "1  300002  96,312,334.3856\n",
       "2  300003 260,533,802.7713\n",
       "3  300004   9,878,918.0471\n",
       "4  300005  14,471,425.5912"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    ID_COL: test_work[ID_COL].values,\n",
    "    TARGET_COL: np.maximum(test_predictions_series.loc[test_work.index].values, 0),\n",
    "})\n",
    "\n",
    "suffix = \"full_retrain\" if USE_FULL_RETRAIN else \"cv_avg\"\n",
    "submission_path = OUTPUT_DIR / f\"submission_lightgbm_monotonic_{suffix}.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "print(f\"Approach: {'Full data retrain' if USE_FULL_RETRAIN else 'CV fold averaging'}\")\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1261d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>district_buildyear_price_median</td>\n",
       "      <td>999,235.0289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Area</td>\n",
       "      <td>152,159.5420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Municipality</td>\n",
       "      <td>115,328.4142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CityPlanning</td>\n",
       "      <td>105,606.8525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NearestStation</td>\n",
       "      <td>88,177.9570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>district_buildyear_price_mean</td>\n",
       "      <td>66,229.3815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DistrictName</td>\n",
       "      <td>49,255.3865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>district_buildyear_price_count_log</td>\n",
       "      <td>29,243.2269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BuildingYear</td>\n",
       "      <td>28,581.8356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Area_log</td>\n",
       "      <td>24,083.3926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TotalFloorArea</td>\n",
       "      <td>20,232.5489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>TimeToNearestStation</td>\n",
       "      <td>17,786.8921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BuildingAge</td>\n",
       "      <td>17,700.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Type</td>\n",
       "      <td>14,870.3694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MunicipalityCode</td>\n",
       "      <td>13,892.5262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breadth</td>\n",
       "      <td>12,903.8654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BuildingYear_missing_flag</td>\n",
       "      <td>12,579.3972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Classification_missing_flag</td>\n",
       "      <td>11,725.5004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Prefecture</td>\n",
       "      <td>11,547.5264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Year</td>\n",
       "      <td>10,759.2583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               feature   importance\n",
       "62     district_buildyear_price_median 999,235.0289\n",
       "0                                 Area 152,159.5420\n",
       "33                        Municipality 115,328.4142\n",
       "8                         CityPlanning 105,606.8525\n",
       "35                      NearestStation  88,177.9570\n",
       "61       district_buildyear_price_mean  66,229.3815\n",
       "16                        DistrictName  49,255.3865\n",
       "60  district_buildyear_price_count_log  29,243.2269\n",
       "6                         BuildingYear  28,581.8356\n",
       "2                             Area_log  24,083.3926\n",
       "52                      TotalFloorArea  20,232.5489\n",
       "50                TimeToNearestStation  17,786.8921\n",
       "5                          BuildingAge  17,700.0127\n",
       "56                                Type  14,870.3694\n",
       "34                    MunicipalityCode  13,892.5262\n",
       "3                              Breadth  12,903.8654\n",
       "7            BuildingYear_missing_flag  12,579.3972\n",
       "11         Classification_missing_flag  11,725.5004\n",
       "37                          Prefecture  11,547.5264\n",
       "59                                Year  10,759.2583"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features = (\n",
    "    feature_importance_summary.groupby(\"feature\", as_index=False)[\"importance\"].mean()\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .head(20)\n",
    ")\n",
    "top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f880d",
   "metadata": {},
   "source": [
    "## 15. Model Comparison & Diagnostics\n",
    "Compare predictions and feature importance between approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16243d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nLocal CV RMSLE: {overall_rmsle_score:.5f}\")\n",
    "print(f\"Training approach: {'Full data retrain' if USE_FULL_RETRAIN else 'CV fold averaging'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEGMENT SCORES\")\n",
    "print(\"=\"*60)\n",
    "for seg_name, seg_result in segment_results.items():\n",
    "    print(f\"\\n{seg_name}:\")\n",
    "    print(f\"  RMSLE: {seg_result['score_mean']:.5f} ± {seg_result['score_std']:.5f}\")\n",
    "    if 'avg_best_iteration' in seg_result:\n",
    "        print(f\"  Avg best iteration: {seg_result['avg_best_iteration']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOOF predictions:\")\n",
    "print(oof_series.describe())\n",
    "print(\"\\nTest predictions:\")\n",
    "print(test_predictions_series.describe())\n",
    "print(\"\\nTrain target:\")\n",
    "print(train_filtered[TARGET_COL].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db3b13",
   "metadata": {},
   "source": [
    "### Experiment Notes\n",
    "**To compare both approaches:**\n",
    "1. Run with `USE_FULL_RETRAIN = True` → generates `submission_lightgbm_monotonic_full_retrain.csv`\n",
    "2. Change to `USE_FULL_RETRAIN = False` → generates `submission_lightgbm_monotonic_cv_avg.csv`\n",
    "3. Submit both to Kaggle and compare public LB scores\n",
    "\n",
    "**Expected differences:**\n",
    "- Full retrain: May have slightly lower LB score but uses all training data\n",
    "- CV average: More robust ensemble, better generalization on unseen data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
